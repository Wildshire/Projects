{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final code_task2.ipynb","provenance":[],"collapsed_sections":["08YIJ_-ddz23","IVPa-5u46QBn","jQAqKHYAVDy1","NmPnjdrhzGMo","FtBFKtvxzPKk","EU6fqQdnVqjS","b_JVbVfXVwf5","m3fBJgC7VzT9","z-Wvg58dvxuU","eK5dpWmaot1w","w5oxRf5Hoxg3","KCR6rvMev-j0","nRLguVFHwQhw","0o5qqwq0pGRX","fzS_Gtk-wT_H","Vcw4TQREwc4C","VRrNAlh1yUZF","yeJd6jFzy0Xz","sW9KWyrqy_Qr"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jgt2LM0wJc19"},"source":["# Task 2 Notebook: The Bug rebuttals\n","This is the complete code for our research in task 2 of the Age perception challenge.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"J0gaY-pZJ5_T"},"source":["# Index\n","- [1. README](#1)\n","- [2. Weighted Samples Research](#2)\n","  - [2.1 Imports](#2-1)\n","  - [2.2 Data Gathering](#2-2)\n","  - [2.3 The models](#2-3)\n","  - [2.4 Training and approach](#2-4)\n","    - [2.4.1 Phase 1](#2-4-1)\n","    - [2.4.2 Phase 2](#2-4-2)\n","    - [2.4.3 Different group weighted sampling](#2-4-3)\n","      - [2.4.3.1 Original](#2-4-3-1)\n","      - [2.4.3.2 48 groups](#2-4-3-2)\n","      - [2.4.3.3 7 Age groups](#2-4-3-3)\n","- [3. Custom Losses Reasearch](#3)\n","  - [3.1 Pre-requisites](#3-1)\n","  - [3.2 Imports](#3-2)\n","  - [3.3 Data Gathering](#3-3)\n","  - [3.4 The Task 1 model](#3-4)\n","  - [3.5 Improving Task 1 results using phase 2 training as a baseline](#3-5)\n","    - [3.5.1 Re-load data and use sample weights](#3-5-1)\n","    - [3.5.2 Defining Custom Losses](#3-5-2)\n","    - [3.5.3 Use sample weights and custom loss to train phase 2](#3-5-3)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JX39x1PjTqB3"},"source":["<a name='1'></a>\n","# README\n","The notebook's purpose is to complement the report for this task. Due to time constraints, we have just organized the notebook so the code is easily found in each section but it is not meant to be executable. This is mostly because we have to come up with different model paths locally in our drives (as the website was down) or for example when we combined our work it was simpler to leave the repeated code there so the context is not lost while reading the report and the code at the same time. "]},{"cell_type":"markdown","metadata":{"id":"VN3yqhCtTzR-"},"source":["<a name='2'></a>\n","# Weighted Samples Research"]},{"cell_type":"markdown","metadata":{"id":"08YIJ_-ddz23"},"source":["<a name='2-1'></a>\n","## Imports"]},{"cell_type":"code","metadata":{"id":"3zD8W4uudz28"},"source":["import h5py\n","import tensorflow as tf\n","import random \n","import pickle\n","import cv2\n","import numpy as np\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","\n","from matplotlib import pyplot as plt\n","\n","from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IVPa-5u46QBn"},"source":["<a name='2-2'></a>\n","## Data Gathering"]},{"cell_type":"markdown","metadata":{"id":"rMULLE13o-Qo"},"source":["### Downloading and decompressing the Appa-Real Age Dataset [(source)](http://chalearnlap.cvc.uab.es/challenge/13/track/13/description/)\n","\n","- As default, RGB images (cropped faces) are in the range of [0, 255], and labels are in the range of ~0.9 to ~90 (years old).\n","- The data is divided in train, validation and test set. \n","- Matadata is also provided\n","  - gender: male / female \n","  - ethnicity: asian / afroamerican / caucasian\n","  - facial expression: neutral / slightlyhappy / happy / other\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8DZFR8GpIfL","executionInfo":{"status":"ok","timestamp":1634376319419,"user_tz":-120,"elapsed":18162,"user":{"displayName":"Jose Gonzalez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14739548051175156468"}},"outputId":"5135fbae-c24c-4da4-a71c-d47d9676098e"},"source":["# downloading the data\n","#!wget https://data.chalearnlap.cvc.uab.cat/Colab_2021/app_data.zip\n","\n","with ZipFile('drive/MyDrive/Computer Vision/Task 2/app_data.zip','r') as zip:\n","   zip.extractall()\n","   print('Data decompressed successfully')\n","\n","# removing the .zip file after extraction to clean space\n","#!rm app_data.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data decompressed successfully\n"]}]},{"cell_type":"markdown","metadata":{"id":"a18X0kFGXUnt"},"source":["### Loading the train/validation data, and re-scaling the labels to [0..1]\n","- X_[train,valid,test] = Face images\n","- Y_[train,valid,test] = Ground truth \n","- M_[train,valid,test] = Metadata (gender, ethnicicy, facial expression)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9GRhB1gzXlYc","executionInfo":{"status":"ok","timestamp":1634376319966,"user_tz":-120,"elapsed":581,"user":{"displayName":"Jose Gonzalez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14739548051175156468"}},"outputId":"11c2593b-41db-42ad-88d7-71036e0cc8ee"},"source":["# loading the train data\n","X_train = np.load('./data/data_train.npy')\n","Y_train = np.load('./data/labels_train.npy')\n","M_train = np.load('./data/meta_data_train.npy')\n","\n","# loading the validation data\n","X_valid = np.load('./data/data_valid.npy')\n","Y_valid = np.load('./data/labels_valid.npy')\n","M_valid = np.load('./data/meta_data_valid.npy')\n","\n","# loading the test data\n","X_test = np.load('./data/data_test.npy')\n","Y_test = np.load('./data/labels_test.npy')\n","M_test = np.load('./data/meta_data_test.npy')\n","\n","# train labels are real numbers, ranging from ~0.9 to ~89 (years old);\n","# we will re-scale the labels to [0,1] by using a normalization factor of 100,\n","# assuming there is no sample with age > 100.\n","Y_train = Y_train/100\n","Y_valid = Y_valid/100\n","# Y_test = Y_test/100 # -> we don't normalize the test labels as we will evaluate \n","                      # them using the raw data, i.e., the apparent age values\n","\n","print('Train data size and shape', X_train.shape)\n","print('Train labels size and shape', Y_train.shape)\n","print('Train metadata size and shape', M_train.shape)\n","print('----')\n","print('Valid data size and shape', X_valid.shape)\n","print('Valid labels size and shape', Y_valid.shape)\n","print('Valid metadata size and shape', M_valid.shape)\n","print('----')\n","print('Test data size and shape', X_test.shape)\n","print('Test labels size and shape', Y_test.shape)\n","print('Test metadata size and shape', M_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train data size and shape (4065, 224, 224, 3)\n","Train labels size and shape (4065,)\n","Train metadata size and shape (4065, 3)\n","----\n","Valid data size and shape (1482, 224, 224, 3)\n","Valid labels size and shape (1482,)\n","Valid metadata size and shape (1482, 3)\n","----\n","Test data size and shape (1978, 224, 224, 3)\n","Test labels size and shape (1978,)\n","Test metadata size and shape (1978, 3)\n"]}]},{"cell_type":"markdown","metadata":{"id":"wMgX5lBgSN3t"},"source":["### Final processing\n","We are going to use Resnet50"]},{"cell_type":"code","metadata":{"id":"1R2XBuG3SQ2C"},"source":["# train\n","for i in range(0,X_train.shape[0]):\n","  x = X_train[i,:,:,:]\n","  x = np.expand_dims(x, axis=0)\n","  X_train[i,] = preprocess_input(x)\n","\n","# validation\n","for i in range(0,X_valid.shape[0]):\n","  x = X_valid[i,:,:,:]\n","  x = np.expand_dims(x, axis=0)\n","  X_valid[i,] = preprocess_input(x)  \n","\n","# test\n","for i in range(0,X_test.shape[0]):\n","  x = X_test[i,:,:,:]\n","  x = np.expand_dims(x, axis=0)\n","  X_test[i,] = preprocess_input(x) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jQAqKHYAVDy1"},"source":["<a name='2-3'></a>\n","## The models\n"]},{"cell_type":"code","metadata":{"id":"IkySaySE9yX-"},"source":["model_1 = False\n","model_2 = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xNKZgM7nyOE0"},"source":["if (model_1):\n","  skip=False  \n","  # downloading the data\n","  #!wget https://data.chalearnlap.cvc.uab.cat/Colab_2021/model.zip\n","\n","  # decompressing the data\n","  with ZipFile('drive/MyDrive/Computer Vision/Task 2/model.zip','r') as zip:\n","    zip.extractall()\n","    print('Model decompressed successfully')\n","\n","  # removing the .zip file after extraction  to clean space\n","  #!rm model.zip\n","\n","  # loading the pretrained model\n","  model = tf.keras.models.load_model('./model/weights.h5')\n","\n","  # Using the FC layer before the 'classifier_low_dim' layer as feature vector\n","  fc_512 = model.get_layer('dim_proj').output\n","\n","  # adding a dropout layer to minimize overfiting problems (or not)\n","  dp_512 = Dropout(0.5)(fc_512)\n","  fc_64 = Dense(64, activation='relu', name='f_64')(dp_512)\n","\n","  # adding a few hidden FC layers to learn hidden representations\n","  #fc_64 = Dense(64, activation='relu', name='f_64')(fc_512)\n","  fc_16 = Dense(16, activation='relu', name='f_16')(fc_64)\n","  fc_8 = Dense(8, activation='relu', name='f_8')(fc_16)\n","\n","  # Includint an additional FC layer with sigmoid activation, used to regress\n","  # the apparent age\n","  output = Dense(1, activation='sigmoid', name='predict')(fc_8)\n","\n","  # building and pringing the final model\n","  model = Model(inputs=model.get_layer('base_input').output,outputs=output)\n","  #print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"STkqOs_4ojmt"},"source":["if (model_2):\n","  skip=True\n","  # loading the pretrained model\n","  model = tf.keras.models.load_model('drive/MyDrive/Computer Vision/Task 2/model_stage1/best_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CclGC67OiS49"},"source":["<a name='2-4'></a>\n","## Training and approach"]},{"cell_type":"markdown","metadata":{"id":"NmPnjdrhzGMo"},"source":["<a name='2-4-1'></a>\n","### Phase 1"]},{"cell_type":"code","metadata":{"id":"HvTNjgWKJ9Aw"},"source":["patience=4\n","lr=0.00001\n","batch=32\n","epochs=80"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7A1WbEC6zNVF"},"source":["if (not skip):  \n","  counter = 0\n","  for layer in model.layers:\n","    if counter <= 174:   #  using 174 or 133\n","      layer.trainable = False\n","    else:\n","      layer.trainable = True\n","    # print(counter, layer.name, layer.trainable)\n","    counter +=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJ52oeITyrIm"},"source":["if (not skip):  \n","  # defining the early stop criteria\n","  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n","  # saving the best model based on val_loss\n","  mc = ModelCheckpoint('/content/drive/MyDrive/temp/best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n","\n","  # defining the optimizer\n","  model.compile(tf.keras.optimizers.Adam(learning_rate=lr), loss=tf.keras.losses.MeanSquaredError(), metrics=['mae']) #loss=tf.keras.losses.MeanSquaredError()loss=tf.keras.losses.Huber()\n","\n","  # training the model\n","  history = model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), batch_size=batch, epochs=epochs, shuffle=True, verbose=1, callbacks=[es,mc])\n","\n","  # saving training history (for future visualization)\n","  with open('/content/drive/MyDrive/temp/train_history.pkl', 'wb') as handle:\n","    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtBFKtvxzPKk"},"source":["<a name='2-4-2'></a>\n","### Phase 2"]},{"cell_type":"code","metadata":{"id":"81OumL6mKWcX"},"source":["patience=4\n","lr=0.00001\n","batch=16\n","epochs=25"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5S5_9qY2zZoS"},"source":["if (model_1):\n","  saved_model = load_model('/content/drive/MyDrive/temp/best_model.h5')\n","elif (model_2):\n","  saved_model=model\n","\n","# setting all layers of the model to trainable\n","saved_model.trainable = True\n","\n","counter = 0\n","for layer in saved_model.layers:\n","  # print(counter, layer.name, layer.trainable)\n","  counter +=1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i9ME5X-CVgYk"},"source":["<a name='2-4-3'></a>\n","### Different group weighted sampling"]},{"cell_type":"code","metadata":{"id":"pR_EMnV24l2Z"},"source":["og=False\n","custom_I=False\n","custom_II=True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EU6fqQdnVqjS"},"source":["<a name='2-4-3-1'></a>\n","#### Original"]},{"cell_type":"code","metadata":{"id":"5eNl9MZ13Ukm"},"source":["if (og): \n","  # counting the number of samples per group in the train data (age attribute only)\n","  g1 = g2 = g3 = g4 = 0\n","  for i in range(0,Y_train.shape[0]):\n","      if(Y_train[i]*100<20):\n","        g1 +=1\n","      if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n","        g2 +=1\n","      if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n","        g3 +=1\n","      if(Y_train[i]*100>=60):\n","        g4 +=1\n","  print('group(s) size = ', [g1, g2, g3, g4])\n","\n","  # generating the weights for each group using the equation defined above\n","  w = sum(np.array([g1, g2, g3, g4]))/(4*np.array([g1, g2, g3, g4]))\n","  print('weights per group = ', w)\n","\n","  # creating a vector with same size as Y_train, that will link a particular label to its weight\n","  sample_weights = []\n","  for i in range(0,Y_train.shape[0]):\n","      if(Y_train[i]*100<20):\n","        sample_weights.append(w[0])\n","      if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n","        sample_weights.append(w[1])\n","      if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n","        sample_weights.append(w[2])\n","      if(Y_train[i]*100>=60):\n","        sample_weights.append(w[3])\n","  sample_weights = np.array(sample_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b_JVbVfXVwf5"},"source":["<a name='2-4-3-2'></a>\n","#### 48 Groups"]},{"cell_type":"code","metadata":{"id":"86Tajzj-4rvI"},"source":["if (custom_I):\n","  #Creating a dictionary\n","  weights={}\n","  for i in range(1,49):\n","    j=str(i)\n","    weights[\"g\"+j]=0\n","  # counting the number of samples per group in the train data:\n","  for i in range(0,Y_train.shape[0]):\n","      #If young\n","      if (Y_train[i]*100<20):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g1\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g2\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g3\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g4\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g5\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g6\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g7\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g8\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g9\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g10\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g11\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g12\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If young 40\n","      elif (Y_train[i]*100<40):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g13\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g14\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g15\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g16\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 40 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g17\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g18\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g19\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g20\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 40 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g21\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g22\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g23\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g24\"]+=1\n","          #ERRORValueError('A very specific bad thing happened.')\n","          else:\n","            raise ValueError('No metadata in 40 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If young 60\n","      elif (Y_train[i]*100<60):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g25\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g26\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g27\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g28\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g29\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g30\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g31\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g32\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g33\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g34\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g35\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g36\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If over 60 40\n","      elif (Y_train[i]*100>=60):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g37\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g38\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g39\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g40\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"41\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g42\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g43\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g44\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g45\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g46\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g47\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g48\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 asian?')\n","        else:\n","          raise ValueError('???')\n","      else:\n","        raise ValueError('IMPOSIBLE')\n","\n","  #We have some groups with 0 values\n","  for i in range(1,49):\n","    j=str(i)\n","    if (weights[\"g\"+j]==0):\n","      weights[\"g\"+j]=1\n","\n","\n","  #print(weights)\n","  # generating the weights for each group using the equation defined above\n","  w = sum(np.array(list(weights.values())))/(48*np.array(list(weights.values())))\n","  #print('weights per group = ', w)\n","\n","  # creating a vector with same size as Y_train, that will link a particular label to its weight\n","  sample_weights = []\n","  for i in range(0,Y_train.shape[0]):\n","      #If young\n","      if (Y_train[i]*100<20):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[0])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[1])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[2])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[3])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[4])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[5])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[6])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[7])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[8])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[9])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[10])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[11])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If young 40\n","      elif (Y_train[i]*100<40):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[12])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[13])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[14])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[15])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 40 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[16])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[17])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[18])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[19])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 40 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[20])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[21])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","           sample_weights.append(w[22])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[23])\n","          #ERRORValueError('A very specific bad thing happened.')\n","          else:\n","            raise ValueError('No metadata in 40 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If young 60\n","      elif (Y_train[i]*100<60):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[24])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[25])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[26])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[27])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[28])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[29])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[30])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[31])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[32])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[33])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[34])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[35])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If over 60 40\n","      elif (Y_train[i]*100>=60):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[36])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[37])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[38])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[39])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[40])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[41])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[42])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[43])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[44])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[45])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[46])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[47])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 asian?')\n","        else:\n","          raise ValueError('???')\n","      else:\n","        raise ValueError('IMPOSIBLE')\n","  sample_weights = np.array(sample_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m3fBJgC7VzT9"},"source":["<a name='2-4-3-3'></a>\n","#### 7 Age groups"]},{"cell_type":"code","metadata":{"id":"M9cQ4PUtnFBE"},"source":["if (custom_II): \n","  # counting the number of samples per group in the train data (age attribute only)\n","  g1 = g2 = g3 = g4 = g5 = g6 = g7 = 0\n","  for i in range(0,Y_train.shape[0]):\n","      if(Y_train[i]*100<10):\n","        g1 +=1\n","      if(Y_train[i]*100>=10 and Y_train[i]*100<20):\n","        g2 +=1\n","      if(Y_train[i]*100>=20 and Y_train[i]*100<30):\n","        g3 +=1\n","      if(Y_train[i]*100>=30 and Y_train[i]*100<40):\n","        g4 +=1\n","      if(Y_train[i]*100>=40 and Y_train[i]*100<50):\n","        g5 +=1\n","      if(Y_train[i]*100>=50 and Y_train[i]*100<60):\n","        g6 +=1\n","      if(Y_train[i]*100>=60):\n","        g7 +=1\n","\n","  # generating the weights for each group using the equation defined above\n","  w = sum(np.array([g1, g2, g3, g4, g5, g6, g7]))/(7*np.array([g1, g2, g3, g4, g5, g6, g7]))\n","  #print('weights per group = ', w)\n","\n","  # creating a vector with same size as Y_train, that will link a particular label to its weight\n","  sample_weights = []\n","  for i in range(0,Y_train.shape[0]):\n","      if(Y_train[i]*100<10):\n","        sample_weights.append(w[0])\n","      if(Y_train[i]*100>=10 and Y_train[i]*100<20):\n","        sample_weights.append(w[1])\n","      if(Y_train[i]*100>=20 and Y_train[i]*100<30):\n","        sample_weights.append(w[2])\n","      if(Y_train[i]*100>=30 and Y_train[i]*100<40):\n","        sample_weights.append(w[3])\n","      if(Y_train[i]*100>=40 and Y_train[i]*100<50):\n","        sample_weights.append(w[4])\n","      if(Y_train[i]*100>=50 and Y_train[i]*100<60):\n","        sample_weights.append(w[5])\n","      if(Y_train[i]*100>=60):\n","        sample_weights.append(w[6])\n","  sample_weights = np.array(sample_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IkMm_h1sWlKf"},"source":["<a name='3'></a>\n","#Custom losses Research"]},{"cell_type":"markdown","metadata":{"id":"z-Wvg58dvxuU"},"source":["<a name='3-1'></a>\n","## Pre-requisites"]},{"cell_type":"code","metadata":{"id":"1hZfDKTPv1Ja"},"source":["!pip install tensorflow\n","!pip install opencv-python\n","!pip install h5py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDGpPFV5v6DG"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eK5dpWmaot1w"},"source":["<a name='3-2'></a>\n","## Imports"]},{"cell_type":"code","metadata":{"id":"Zp1yczENv8eM"},"source":["import h5py\n","import tensorflow as tf\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","from matplotlib import pyplot as plt\n","import random \n","import pickle\n","import cv2\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w5oxRf5Hoxg3"},"source":["<a name='3-3'></a>\n","## Data Gathering"]},{"cell_type":"markdown","metadata":{"id":"KCR6rvMev-j0"},"source":["### Loading the train/validation data, and re-scaling the labels to [0..1]"]},{"cell_type":"code","metadata":{"id":"VuNPzDMHwBjZ"},"source":["# downloading the data\n","#!wget https://data.chalearnlap.cvc.uab.cat/Colab_2021/app_data.zip\n","\n","# decompressing the data\n","from zipfile import ZipFile\n","\n","with ZipFile('drive/MyDrive/app_data.zip','r') as zip:\n","   zip.extractall()\n","   print('Data decompressed successfully')\n","\n","# removing the .zip file after extraction to clean space\n","#!rm app_data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gJWyaa1wM72"},"source":["import numpy as np\n","\n","# loading the train data\n","X_train = np.load('./data/data_train.npy')\n","Y_train = np.load('./data/labels_train.npy')\n","M_train = np.load('./data/meta_data_train.npy')\n","\n","# loading the validation data\n","X_valid = np.load('./data/data_valid.npy')\n","Y_valid = np.load('./data/labels_valid.npy')\n","M_valid = np.load('./data/meta_data_valid.npy')\n","\n","# loading the test data\n","X_test = np.load('./data/data_test.npy')\n","Y_test = np.load('./data/labels_test.npy')\n","M_test = np.load('./data/meta_data_test.npy')\n","\n","\n","Y_train = Y_train/100\n","Y_valid = Y_valid/100\n","# Y_test = Y_test/100 \n","\n","print('Train data size and shape', X_train.shape)\n","print('Train labels size and shape', Y_train.shape)\n","print('Train metadata size and shape', M_train.shape)\n","print('----')\n","print('Valid data size and shape', X_valid.shape)\n","print('Valid labels size and shape', Y_valid.shape)\n","print('Valid metadata size and shape', M_valid.shape)\n","print('----')\n","print('Test data size and shape', X_test.shape)\n","print('Test labels size and shape', Y_test.shape)\n","print('Test metadata size and shape', M_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRLguVFHwQhw"},"source":["### Preprocessing the data (face images)"]},{"cell_type":"code","metadata":{"id":"sgcqZiEQwRX4"},"source":["from tensorflow.keras.applications.resnet50 import preprocess_input\n","\n","# train\n","for i in range(0,X_train.shape[0]):\n","  x = X_train[i,:,:,:]\n","  x = np.expand_dims(x, axis=0)\n","  X_train[i,] = preprocess_input(x)\n","\n","# validation\n","for i in range(0,X_valid.shape[0]):\n","  x = X_valid[i,:,:,:]\n","  x = np.expand_dims(x, axis=0)\n","  X_valid[i,] = preprocess_input(x)  \n","\n","# test\n","for i in range(0,X_test.shape[0]):\n","  x = X_test[i,:,:,:]\n","  x = np.expand_dims(x, axis=0)\n","  X_test[i,] = preprocess_input(x) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0o5qqwq0pGRX"},"source":["<a name='3-4'></a>\n","## The Task 1 model"]},{"cell_type":"markdown","metadata":{"id":"fzS_Gtk-wT_H"},"source":["### Downloading the ResNet50 model pre-trained on Faces"]},{"cell_type":"code","metadata":{"id":"hnh9moOMwYHl"},"source":["# downloading the data\n","!wget https://data.chalearnlap.cvc.uab.cat/Colab_2021/model.zip\n","\n","# decompressing the data\n","with ZipFile('drive/MyDrive/model.zip','r') as zip:\n","   zip.extractall()\n","   print('Model decompressed successfully')\n","\n","# removing the .zip file after extraction  to clean space\n","!rm model.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vcw4TQREwc4C"},"source":["### Loading the pre-trained model"]},{"cell_type":"code","metadata":{"id":"rzBS88HewfzK"},"source":["# loading the pretrained model\n","model = tf.keras.models.load_model('./model/weights.h5')\n","\n","# print the model summary\n","#print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jAy320InwlGF"},"source":["# Using the FC layer before the 'classifier_low_dim' layer as feature vector\n","fc_512 = model.get_layer('dim_proj').output\n","\n","# adding a dropout layer to minimize overfiting problems\n","#dp_layer = Dropout(0.5)(fc_512)\n","\n","# adding a few hidden FC layers to learn hidden representations\n","#fc_128 = Dense(128, activation='relu', name='f_128')(fc_512)\n","#fc_32 = Dense(32, activation='relu', name='f_32')(fc_128)\n","\n","fc_64 = Dense(64, activation='relu', name='f_64')(fc_512)\n","fc_16 = Dense(16, activation='relu', name='f_16')(fc_64)\n","fc_8 = Dense(8, activation='relu', name='f_8')(fc_16)\n","\n","# Includint an additional FC layer with sigmoid activation, used to regress\n","# the apparent age\n","output = Dense(1, activation='sigmoid', name='predict')(fc_8)\n","\n","# building and pringing the final model\n","model = Model(inputs=model.get_layer('base_input').output,outputs=output)\n","#print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0GTun0ZHxMBf"},"source":["### Using the best parameters, model  architecture achieved in task 1 to train the resenet50 for stage 1"]},{"cell_type":"code","metadata":{"id":"enpoQ-OJxIah"},"source":["counter = 0\n","for layer in model.layers:\n","  if counter <= 174: \n","    layer.trainable = False\n","  else:\n","    layer.trainable = True\n","  print(counter, layer.name, layer.trainable)\n","  counter +=1\n","#print(model.summary())\n","\n","\n","# defining the early stop criteria\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n","# saving the best model based on val_loss\n","mc = ModelCheckpoint('/content/gdrive/MyDrive/temp/best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n","\n","# defining the optimizer\n","model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5),loss=tf.keras.losses.MeanSquaredError(),metrics=['mae'])\n","\n","# training the model\n","history = model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), batch_size=32, epochs=30, shuffle=True, verbose=1, callbacks=[es,mc])\n","\n","# saving training history (for future visualization)\n","with open('/content/gdrive/MyDrive/temp/train_history.pkl', 'wb') as handle:\n","  pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","\n","saved_model = load_model('/content/gdrive/MyDrive/temp/best_model.h5')\n","\n","\n","# predict on the test data\n","predictions_st1 = saved_model.predict(X_test, batch_size=32, verbose=1)\n","\n","predictions_st1_f = predictions_st1*100\n","\n","# evaluating on test data\n","error = []\n","for i in range(0,len(Y_test)):\n","  error.append(abs(np.subtract(predictions_st1_f[i][0],Y_test[i])))\n","\n","print('MAE = %.8f' %(np.mean(error)))\n","\n","for i in range(0,10):\n","    print('predicted age = %.3f - Ground truth = %.3f' %(predictions_st1_f[i], Y_test[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4rrZePBHxlaj"},"source":["### Using the best parameters, model  architecture achieved in task 1 to train the resenet50 for stage 2 without custom loss"]},{"cell_type":"code","metadata":{"id":"do7jvargxvIk"},"source":["# setting all layers of the model to trainable\n","saved_model.trainable = True\n","\n","counter = 0\n","for layer in saved_model.layers:\n","  print(counter, layer.name, layer.trainable)\n","  counter +=1\n","\n","# training all layers (2nd stage), given the model saved on stage 1\n","saved_model.compile(tf.keras.optimizers.Adam(learning_rate=5e-5),loss=tf.keras.losses.MeanSquaredError(),metrics=['mae'])\n","\n","# defining the early stop criteria\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n","# saving the best model (2nd stage) based on val_loss with a different name\n","mc = ModelCheckpoint('/content/gdrive/MyDrive/temp/best_model_2nd_stage.h5', monitor='val_loss', mode='min', save_best_only=True)\n","\n","history = saved_model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), batch_size=16, epochs=10, shuffle=True, verbose=1, callbacks=[es,mc])\n","\n","# saving training history\n","with open('/content/gdrive/MyDrive/temp/train_history_2nd_stage.pkl', 'wb') as handle:\n","  pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","train_hist = pickle.load(open('/content/gdrive/MyDrive/temp/train_history.pkl',\"rb\"))\n","train_hist_2nd = pickle.load(open('/content/gdrive/MyDrive/temp/train_history_2nd_stage.pkl',\"rb\"))\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n","fig.suptitle('Training history (Stage 1 and Stage 2)', fontsize=14, fontweight='bold')\n","\n","ax1.plot(train_hist['loss']+train_hist_2nd['loss'])\n","ax1.plot(train_hist['val_loss']+train_hist_2nd['val_loss'])\n","ax1.axvline(42, 0, 1, ls='--', color='r')\n","ax1.set(xlabel='epoch', ylabel='Loss')\n","ax1.legend(['train', 'valid'], loc='upper right')\n","\n","ax2.plot(train_hist['mae']+train_hist_2nd['mae'])\n","ax2.plot(train_hist['val_mae']+train_hist_2nd['val_mae'])\n","ax2.axvline(42, 0, 1, ls='--', color='r')\n","ax2.set(xlabel='epoch', ylabel='MAE')\n","ax2.legend(['train', 'valid'], loc='upper right')\n","\n","saved_model_2nd = load_model('/content/gdrive/MyDrive/temp/best_model_2nd_stage.h5')\n","\n","\n","\n","# predict on the test data\n","predictions_2nd = saved_model_2nd.predict(X_test, batch_size=32, verbose=1)\n","predictions_2nd_f = predictions_2nd*100\n","error = []\n","for i in range(0,len(Y_test)):\n","  error.append(abs(np.subtract(predictions_2nd_f[i][0],Y_test[i])))\n","\n","print('MAE = %.8f' %(np.mean(error)))\n","# printing some predictions\n","for i in range(0,10):\n","  print('predicted age = %.3f - Ground truth = %.3f' %(predictions_2nd_f[i], Y_test[i]))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U7dadtKrrsPf"},"source":["<a name='3-5'></a>\n","## Improving Task 1 results using phase 2 as baseline"]},{"cell_type":"markdown","metadata":{"id":"VRrNAlh1yUZF"},"source":["<a name='3-5-1'></a>\n","### Load the Train data again  and generate the weigths \n","\n","We are only going to use the first two sample weights"]},{"cell_type":"code","metadata":{"id":"tURGEgLUyYjI"},"source":["# loading the train data again (original face images, before preprocessing):\n","X_train = np.load('./data/data_train.npy')\n","Y_train = np.load('./data/labels_train.npy')\n","Y_train = Y_train/100 # normalizing the age values to be between [0,1]\n","\n","# preprocessing the train data with respect to ResNet-50 Inputs.\n","for i in range(0,X_train.shape[0]):\n","  x = X_train[i,:,:,:]\n","  x = np.expand_dims(x, axis=0)\n","  X_train[i,] = preprocess_input(x)\n","\n","# counting the number of samples per group in the train data (age attribute only)\n","g1 = g2 = g3 = g4 = 0\n","for i in range(0,Y_train.shape[0]):\n","    if(Y_train[i]*100<20):\n","      g1 +=1\n","    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n","      g2 +=1\n","    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n","      g3 +=1\n","    if(Y_train[i]*100>=60):\n","      g4 +=1\n","print('group(s) size = ', [g1, g2, g3, g4])\n","\n","# generating the weights for each group using the equation defined above\n","w = sum(np.array([g1, g2, g3, g4]))/(4*np.array([g1, g2, g3, g4]))\n","print('weights per group = ', w)\n","\n","# creating a vector with same size as Y_train, that will link a particular label to its weight\n","sample_weights = []\n","for i in range(0,Y_train.shape[0]):\n","    if(Y_train[i]*100<20):\n","      sample_weights.append(w[0])\n","    if(Y_train[i]*100>=20 and Y_train[i]*100<40):\n","      sample_weights.append(w[1])\n","    if(Y_train[i]*100>=40 and Y_train[i]*100<60):\n","      sample_weights.append(w[2])\n","    if(Y_train[i]*100>=60):\n","      sample_weights.append(w[3])\n","sample_weights = np.array(sample_weights)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p-UgoonrevzE"},"source":["custome_I=True\n","if custom_I=True:\n","  #Creating a dictionary\n","  weights={}\n","  for i in range(1,49):\n","    j=str(i)\n","    weights[\"g\"+j]=0\n","  # counting the number of samples per group in the train data:\n","  for i in range(0,Y_train.shape[0]):\n","      #If young\n","      if (Y_train[i]*100<20):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g1\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g2\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g3\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g4\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g5\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g6\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g7\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g8\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g9\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g10\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g11\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g12\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If young 40\n","      elif (Y_train[i]*100<40):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g13\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g14\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g15\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g16\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 40 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g17\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g18\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g19\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g20\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 40 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g21\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g22\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g23\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g24\"]+=1\n","          #ERRORValueError('A very specific bad thing happened.')\n","          else:\n","            raise ValueError('No metadata in 40 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If young 60\n","      elif (Y_train[i]*100<60):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g25\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g26\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g27\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g28\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g29\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g30\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g31\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g32\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g33\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g34\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g35\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g36\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If over 60 40\n","      elif (Y_train[i]*100>=60):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g37\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g38\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g39\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g40\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"41\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g42\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g43\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g44\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            weights[\"g45\"]+=1\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            weights[\"g46\"]+=1\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            weights[\"g47\"]+=1\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            weights[\"g48\"]+=1\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 asian?')\n","        else:\n","          raise ValueError('???')\n","      else:\n","        raise ValueError('IMPOSIBLE')\n","\n","  #We have some groups with 0 values\n","  for i in range(1,49):\n","    j=str(i)\n","    if (weights[\"g\"+j]==0):\n","      weights[\"g\"+j]=1\n","\n","\n","  #print(weights)\n","  # generating the weights for each group using the equation defined above\n","  w = sum(np.array(list(weights.values())))/(48*np.array(list(weights.values())))\n","  #print('weights per group = ', w)\n","\n","  # creating a vector with same size as Y_train, that will link a particular label to its weight\n","  sample_weights = []\n","  for i in range(0,Y_train.shape[0]):\n","      #If young\n","      if (Y_train[i]*100<20):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[0])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[1])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[2])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[3])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[4])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[5])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[6])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[7])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[8])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[9])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[10])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[11])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 20 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If young 40\n","      elif (Y_train[i]*100<40):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[12])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[13])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[14])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[15])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 40 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[16])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[17])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[18])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[19])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 40 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[20])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[21])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","           sample_weights.append(w[22])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[23])\n","          #ERRORValueError('A very specific bad thing happened.')\n","          else:\n","            raise ValueError('No metadata in 40 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If young 60\n","      elif (Y_train[i]*100<60):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[24])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[25])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[26])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[27])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[28])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[29])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[30])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[31])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[32])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[33])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[34])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[35])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in 60 asian?')\n","        else:\n","          raise ValueError('???')\n","      #If over 60 40\n","      elif (Y_train[i]*100>=60):\n","        #If caucassian\n","        if (str(M_train[i][1]).lower()==\"caucasian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[36])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[37])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[38])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[39])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 caucasian?')\n","        elif (str(M_train[i][1]).lower()==\"afroamerican\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[40])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[41])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[42])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[43])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 afroamerican?')\n","        elif (str(M_train[i][1]).lower()==\"asian\"):\n","          #If neutral\n","          if (str(M_train[i][2]).lower()==\"neutral\"):\n","            sample_weights.append(w[44])\n","          #If slightly happy\n","          elif (str(M_train[i][2]).lower()==\"slightlyhappy\"):\n","            sample_weights.append(w[45])\n","          #If happy\n","          elif (str(M_train[i][2]).lower()==\"happy\"):\n","            sample_weights.append(w[46])\n","          #If other\n","          elif (str(M_train[i][2]).lower()==\"other\"):\n","            sample_weights.append(w[47])\n","          #ERROR\n","          else:\n","            raise ValueError('No metadata in over 60 asian?')\n","        else:\n","          raise ValueError('???')\n","      else:\n","        raise ValueError('IMPOSIBLE')\n","  sample_weights = np.array(sample_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yeJd6jFzy0Xz"},"source":["<a name='3-5-2'></a>\n","### Define the custom losses\n","Below are the implementations of the weighted focal mse loss, the weighted huber loss and the weighted quantile loss."]},{"cell_type":"code","metadata":{"id":"C83Nox7Vy297"},"source":["def weighted_focal_mse_loss(y_true, y_pred, alpha=1., gamma=2, weights=sample_weights):\n","    #loss = tf.square(y_true - y_pred) \n","    \n","    loss =alpha*tf.keras.backend.pow(tf.square(y_true - y_pred),gamma) \n","    if weights is not None:\n","        loss *= weights\n","    loss = tf.keras.backend.mean(loss)\n","    \n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UGX8gY6y57x"},"source":["def weighted_huber_loss(y_true, y_pred,  delta=1, weights=sample_weights):\n","    l1_loss = tf.keras.backend.abs(y_true - y_pred)\n","\n","    loss = tf.where(l1_loss < delta, 0.5 * (l1_loss ** 2), 0.5 * delta**2 + delta * (l1_loss - delta))\n","    if weights is not None:\n","        loss *= weights\n","    loss = tf.keras.backend.mean(loss)\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rK3K76suy8Xl"},"source":["def weighted_quantile_loss(y_true, y_pred,  gamma=0.7, weights=sample_weights):\n","    #l1_loss = tf.keras.backend.abs(y_true - y_pred)\n","    \n","    loss=gamma*tf.keras.backend.maximum(0.,y_true - y_pred)+(1-gamma)*tf.keras.backend.maximum(0.,y_pred-y_true)\n","    \n","    if weights is not None:\n","        loss *= weights\n","    loss = tf.keras.backend.mean(loss)\n","    return loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sW9KWyrqy_Qr"},"source":["<a name='3-5-3'></a>\n","### Using the sample weights and custom loss to train our model in phase 2\n"]},{"cell_type":"code","metadata":{"id":"5mgokVUlzHrH"},"source":["\n","\n","NUM_EPOCHS = 10\n","\n","RESUME_TRAINING = False\n","\n","\n","if(RESUME_TRAINING == False):\n","  saved_model = load_model('/content/gdrive/MyDrive/temp/best_model.h5') # load model from stage 1\n","else:\n","  # resume training (stage 2)\n","  saved_model = load_model('/content/gdrive/MyDrive/temp/best_model_2nd_stage_weighted.h5')\n","  \n","# setting all layers to traineble\n","saved_model.trainable = True\n","\n","#=================================================\n","# training all layers (2nd stage), given the model saved on stage 1\n","#saved_model.compile(tf.keras.optimizers.Adam(learning_rate=1e-5),loss=tf.keras.losses.MeanSquaredError(),metrics=['mae'])\n","saved_model.compile(tf.keras.optimizers.Adam(learning_rate=5e-5),loss=weighted_huber_loss,metrics=['mae'])\n","#=================================================\n","\n","# defining the early stop criteria\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n","mc = ModelCheckpoint('/content/gdrive/MyDrive/temp/best_model_2nd_stage_weighted.h5', monitor='val_loss', mode='min', save_best_only=True)\n","\n","if(RESUME_TRAINING == False):  \n","  #history = saved_model.fit(X_train, Y_train, sample_weight=sample_weights, validation_data=(X_valid, Y_valid), batch_size=16, epochs=NUM_EPOCHS, initial_epoch=0, shuffle=True, verbose=1, callbacks=[es,mc])\n","  history = saved_model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), batch_size=16, epochs=NUM_EPOCHS, initial_epoch=0, shuffle=True, verbose=1, callbacks=[es,mc])\n","  \n","else:\n","\n","  #history = saved_model.fit(X_train, Y_train, sample_weight=sample_weights, validation_data=(X_valid, Y_valid), batch_size=16, epochs=NUM_EPOCHS, initial_epoch=RESUME_FROM_EPOCH, shuffle=True, verbose=1, callbacks=[es,mc])\n","  history = saved_model.fit(X_train, Y_train,  validation_data=(X_valid, Y_valid), batch_size=16, epochs=NUM_EPOCHS, initial_epoch=RESUME_FROM_EPOCH, shuffle=True, verbose=1, callbacks=[es,mc])\n","# saving training history\n","with open('/content/gdrive/MyDrive/temp/train_history_2nd_stage_weighted.pkl', 'wb') as handle:\n","  pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","  "],"execution_count":null,"outputs":[]}]}