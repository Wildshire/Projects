{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab exercise: Real NVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples1-2 shape must be (n samples, 2)\n",
    "def plot_samples(samples1, samples2=None):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.scatter(samples1[:,0], samples1[:,1], marker=\"x\", color=\"blue\")\n",
    "    if samples2 is not None:\n",
    "        ax.scatter(samples2[:,0], samples2[:,1], marker=\"x\", color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "target_samples, target_classes = sklearn.datasets.make_moons(1000, noise=0.1)\n",
    "target_samples = torch.from_numpy(target_samples).float()\n",
    "\n",
    "plot_samples(target_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVPLayer(nn.Module):\n",
    "    def __init__(self, size, reverse=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mask = torch.zeros(size, requires_grad=False)\n",
    "        mid = int(size / 2)\n",
    "        if reverse:\n",
    "            self.mask[mid:] = 1.\n",
    "        else:\n",
    "            self.mask[:mid] = 1.\n",
    "        \n",
    "        ## the two operations\n",
    "        self.scale = nn.Sequential(\n",
    "            nn.Linear(size, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, size),\n",
    "        )\n",
    "        self.transpose = nn.Sequential(\n",
    "            nn.Linear(size, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, size),\n",
    "        )\n",
    "        \n",
    "    # project from the latent space to the observed space,\n",
    "    # i.e. x = g(z)\n",
    "    def forward(self, z):\n",
    "        # you will need this!\n",
    "        n_mask = 1. - self.mask\n",
    "        z_masked = z * self.mask\n",
    "        \n",
    "        x = #TODO\n",
    "        return x\n",
    "    \n",
    "    # project from the observed space to the latent space,\n",
    "    # this function also return the log det jacobian of this inv function\n",
    "    def inv(self, x):\n",
    "        # you will need this!\n",
    "        n_mask = 1. - self.mask\n",
    "        x_masked = x * self.mask\n",
    "        \n",
    "        # BEGIN TODO\n",
    "        z = # TODO\n",
    "        log_det_jacobian = # TODO\n",
    "        # END TODO\n",
    "        \n",
    "        return z, log_det_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test!\n",
    "layer = RealNVPLayer(2, reverse=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.rand(1, 2)\n",
    "    z, _ = layer.inv(x)\n",
    "    xx = layer(z)\n",
    "\n",
    "    print(\"In the 3 vectors below, the first element must be equal\")\n",
    "    print(\"This two vectors should be equal:\")\n",
    "    print(x)\n",
    "    print(xx)\n",
    "    print(\"This vector should be different to the two above\")\n",
    "    print(z)\n",
    "print()\n",
    "\n",
    "layer = RealNVPLayer(2, reverse=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.rand(1, 2)\n",
    "    z, _ = layer.inv(x)\n",
    "    xx = layer(z)\n",
    "\n",
    "    print(\"In the 3 vectors below, the second element must be equal\")\n",
    "    print(\"This two vectors should be equal:\")\n",
    "    print(x)\n",
    "    print(xx)\n",
    "    print(\"This vector should be different to the two above\")\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, size, n_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.prior = torch.distributions.normal.Normal(torch.zeros(2), torch.ones(2))\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "                            RealNVPLayer(size, i % 2 == 0)\n",
    "                            for i in range(n_layers)\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = z\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "        return x\n",
    "    \n",
    "    def inv(self, x):\n",
    "        log_det_jacobian = 0.\n",
    "        z = x\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            z, j = self.layers[i].inv(z)\n",
    "            # remember here, we just have to sum all log det jacobians!\n",
    "            log_det_jacobian = log_det_jacobian + j\n",
    "        return z, log_det_jacobian\n",
    "\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        z = self.prior.sample((n_samples,))\n",
    "        x = self(z)\n",
    "        return x\n",
    "\n",
    "    def log_prior(self, z):\n",
    "        x, det = self.inv(z)\n",
    "        ret = self.prior.log_prob(x).sum(1) + det\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_distrib = RealNVP(2, 50)\n",
    "optimizer = torch.optim.Adam(trained_distrib.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 1000\n",
    "losses = list()\n",
    "for _ in range(500):\n",
    "    for i in range(0, target_samples.shape[0], batch_size):\n",
    "        batch = target_samples[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = -trained_distrib.log_prior(batch).mean()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(trained_distrib.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    \n",
    "plt.plot(np.arange(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "with torch.no_grad():\n",
    "    samples = trained_distrib.sample(1000)\n",
    "    plot_samples(target_samples, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the latent space corresponding to each half moon in a different color\n",
    "with torch.no_grad():\n",
    "    source_sample1, _ = trained_distrib.inv(target_samples[target_classes == 0])\n",
    "    source_sample2, _ = trained_distrib.inv(target_samples[target_classes == 1])\n",
    "    plot_samples(source_sample1, source_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
